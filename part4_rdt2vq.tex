%==============================================================================
% PART 4: RDT2-VQ ARCHITECTURE
%==============================================================================

\section{RDT2-VQ Architecture}

%------------------------------------------------------------------------------
\begin{frame}{RDT2-VQ Overview}
    \begin{block}{Definition}
        \textbf{RDT2-VQ} = Vision-Language-Action model with VQ action tokenization
    \end{block}

    \vspace{0.5cm}
    \textbf{Core Architecture:}
    \begin{itemize}
        \item \textbf{Backbone:} Qwen2.5-VL-7B-Instruct (fine-tuned)
        \item \textbf{Action Output:} Discrete tokens via VQVAE
        \item \textbf{Generation:} Autoregressive (27 tokens)
    \end{itemize}

    \vspace{0.5cm}
    \begin{alertblock}{Key Characteristic}
        \textbf{27 forward passes} per action chunk (one per token)
    \end{alertblock}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Qwen2.5-VL-7B-Instruct}
    \begin{columns}[T]
        \begin{column}{0.55\textwidth}
            \begin{block}{Base Model}
                Alibaba's VLM with instruction following
            \end{block}

            \vspace{0.2cm}
            \textbf{Model Specifications:}
            \begin{table}
                \centering
                \footnotesize
                \renewcommand{\arraystretch}{1.2}
                \begin{tabular}{ll}
                    \toprule
                    \textbf{Parameter} & \textbf{Value} \\
                    \midrule
                    Parameters & 7B \\
                    Hidden size & 3584 \\
                    Num layers & 28 \\
                    Vocabulary size & 152,064 \\
                    \bottomrule
                \end{tabular}
            \end{table}
        \end{column}
        \begin{column}{0.45\textwidth}
            \textbf{Key Features:}
            \begin{itemize}
                \item Multi-image support
                \item Flash Attention 2
                \item Instruction-tuned
                \item ViT vision encoder
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{RDT2-VQ Architecture Diagram}
    \begin{center}
    \begin{tikzpicture}[
        box/.style={rectangle, draw, rounded corners, minimum width=1.8cm, minimum height=0.6cm, align=center, font=\small},
        arrow/.style={->, thick},
        scale=0.65, transform shape
    ]
        % Input
        \node[box, fill=blue!20] (img_l) at (0,2.5) {Left\\Image};
        \node[box, fill=blue!20] (img_r) at (2.3,2.5) {Right\\Image};
        \node[box, fill=green!20] (text) at (4.6,2.5) {Language\\Instruction};

        % Concatenate
        \node[box, fill=gray!30] (concat) at (1.15,1) {Concat\\$384 \times 768$};

        % Qwen processor
        \node[box, fill=qwenblue!30, minimum width=7cm] (qwen) at (3.5,-0.5) {Qwen2.5-VL-7B-Instruct};

        % Output
        \node[box, fill=vqpurple!30] (tokens) at (3.5,-2) {Action Tokens $[27]$};

        % VAE decode
        \node[box, fill=decodergreen!30] (vae) at (3.5,-3.5) {VQVAE Decode};

        % Final output
        \node[box, fill=actionorange!30] (action) at (3.5,-5) {Action $[24, 20]$};

        % Arrows
        \draw[arrow] (img_l) -- (concat);
        \draw[arrow] (img_r) -- (concat);
        \draw[arrow] (concat) -- (qwen);
        \draw[arrow] (text) -- (qwen);
        \draw[arrow] (qwen) -- node[right, font=\tiny] {autoregressive} (tokens);
        \draw[arrow] (tokens) -- (vae);
        \draw[arrow] (vae) -- (action);
    \end{tikzpicture}
    \end{center}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Image Input Processing (1/2)}
    \textbf{Binocular Fisheye Images:}

    \vspace{0.3cm}
    \begin{center}
    \begin{tikzpicture}[
        img/.style={rectangle, draw, minimum width=2cm, minimum height=2cm, align=center},
        arrow/.style={->, thick}
    ]
        \node[img, fill=blue!10] (left) at (0,0) {Left\\$384 \times 384$};
        \node[img, fill=blue!10] (right) at (3,0) {Right\\$384 \times 384$};
        \node at (1.5,0) {$+$};
        \node[img, fill=green!10, minimum width=4.5cm] (concat) at (8,0) {Concatenated\\$384 \times 768$};

        \draw[arrow] (4.5,0) -- (5.5,0);
    \end{tikzpicture}
    \end{center}

    \begin{block}{Camera Order}
        Left image $\rightarrow$ Right image (spatial alignment)
    \end{block}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Image Input Processing (2/2)}
    \textbf{Processing Steps:}
    \begin{enumerate}
        \item Camera captures at $1280 \times 1024$
        \item Resize to $384 \times 384$ per camera
        \item Concatenate horizontally: $384 \times 768 \times 3$
        \item Optional JPEG compression (training augmentation)
    \end{enumerate}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{Chat Template Format (1/2)}
    \textbf{Qwen2.5-VL Chat Structure:}

    \vspace{0.3cm}
    \begin{lstlisting}[language=Python]
messages = [{
    "role": "user",
    "content": [
        {"type": "image", "image": concatenated_image},
        {"type": "text", "text": instruction}
    ]
}]

text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
text += "<|im_start|>assistant\n<|quad_start|>"
    \end{lstlisting}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Chat Template Format (2/2)}
    \textbf{Special Tokens:}

    \vspace{0.5cm}
    \begin{table}
        \centering
        \begin{tabular}{ll}
            \toprule
            \textbf{Token} & \textbf{Purpose} \\
            \midrule
            \code{<|im\_start|>} & Message start marker \\
            \code{<|im\_end|>} & Message end marker \\
            \code{<|quad\_start|>} & Action sequence start \\
            \code{<|quad\_end|>} & Action sequence end \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Token Sequence Structure}
    \textbf{Complete Input Sequence:}

    \vspace{0.3cm}
    \begin{center}
    \begin{tikzpicture}[
        tok/.style={rectangle, draw, minimum width=1.2cm, minimum height=0.6cm, align=center, font=\scriptsize},
        arrow/.style={->, thick}
    ]
        \node[tok, fill=gray!30] (sys) at (0,0) {System};
        \node[tok, fill=blue!20] (img) at (1.8,0) {Image\\Tokens};
        \node[tok, fill=green!20] (text) at (3.6,0) {Text\\Tokens};
        \node[tok, fill=yellow!30] (start) at (5.4,0) {quad\\start};
        \node[tok, fill=vqpurple!30] (act) at (7.2,0) {Action\\Tokens};
        \node[tok, fill=yellow!30] (end) at (9,0) {quad\\end};

        \draw[decorate, decoration={brace, amplitude=5pt, raise=2pt}]
            (sys.north west) -- (start.north east) node[midway, above=8pt] {Input Context};

        \draw[decorate, decoration={brace, amplitude=5pt, raise=2pt}]
            (act.north west) -- (end.north east) node[midway, above=8pt] {Generated};
    \end{tikzpicture}
    \end{center}

    \vspace{0.5cm}
    \textbf{Token Counts:}
    \begin{itemize}
        \item Image tokens: $\sim$1000 (depends on resolution)
        \item Text tokens: Variable (instruction length)
        \item Action tokens: Exactly 27
        \item Total context: $< 2048$ tokens
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Autoregressive Generation (1/2)}
    \begin{block}{Process}
        Generate 27 action tokens one at a time
    \end{block}

    \vspace{0.3cm}
    \textbf{Generation Loop:}
    \begin{algorithmic}[1]
        \State $\vect{h} \leftarrow \text{Encode}(\text{image}, \text{instruction})$
        \State $\vect{t} \leftarrow [\;]$ \Comment{token sequence}
        \For{$i = 1$ to $27$}
            \State $p_i \leftarrow \text{LMHead}(\vect{h})$ \Comment{logits over vocabulary}
            \State $t_i \leftarrow \argmax(p_i)$ \Comment{greedy decoding}
            \State $\vect{t}.\text{append}(t_i)$
            \State $\vect{h} \leftarrow \text{Update}(\vect{h}, t_i)$ \Comment{KV cache update}
        \EndFor
        \State \textbf{Return} $\vect{t}$
    \end{algorithmic}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Autoregressive Generation (2/2)}
    \textbf{Inference Configuration:}
    \begin{itemize}
        \item Temperature: $0.0$ (greedy decoding)
        \item Max tokens: 29 (27 action + 2 markers)
        \item Sampling: Deterministic (argmax)
    \end{itemize}

    \vspace{0.5cm}
    \begin{alertblock}{Computational Cost}
        \begin{itemize}
            \item 27 sequential forward passes required
            \item Cannot parallelize (autoregressive dependency)
            \item KV cache reduces redundant computation
        \end{itemize}
    \end{alertblock}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Autoregressive Token Generation: Visual Timeline}
    \begin{center}
    \begin{tikzpicture}[scale=0.85, transform shape]
        % Title for timeline
        \node[font=\small\bfseries] at (0,3) {27 Sequential Token Generation Steps};

        % Timeline base
        \draw[thick, gray] (-5.5,1.5) -- (5.5,1.5);

        % Token generation steps - show 9 representative tokens
        \foreach \i/\x/\label in {1/-5/t_1, 2/-4/t_2, 3/-3/t_3, 9/-1/t_9, 14/1/t_{14}, 19/3/t_{19}, 25/4/t_{25}, 26/4.8/t_{26}, 27/5.6/t_{27}} {
            % Token circle
            \fill[vqpurple!70] (\x,1.5) circle (5pt);
            \node[font=\tiny, below] at (\x,1.2) {$\label$};
        }

        % Ellipsis for skipped tokens
        \node[font=\small] at (-2,1.5) {...};
        \node[font=\small] at (0,1.5) {...};
        \node[font=\small] at (2,1.5) {...};

        % Arrows showing autoregressive dependency
        \draw[->, thick, qwenblue, opacity=0.7] (-4.7,1.7) to[bend left=30] (-4.3,1.7);
        \draw[->, thick, qwenblue, opacity=0.7] (-3.7,1.7) to[bend left=30] (-3.3,1.7);

        % KV Cache accumulation visualization
        \node[font=\scriptsize\bfseries, qwenblue] at (-5,0.3) {KV Cache:};

        % Growing KV cache bars
        \fill[qwenblue!30] (-4.5,-0.2) rectangle (-4.3,0);
        \fill[qwenblue!40] (-3.5,-0.2) rectangle (-3.1,0.1);
        \fill[qwenblue!50] (-2.5,-0.2) rectangle (-1.9,0.2);
        \fill[qwenblue!60] (-0.5,-0.2) rectangle (0.3,0.35);
        \fill[qwenblue!70] (1.5,-0.2) rectangle (2.5,0.5);
        \fill[qwenblue!80] (3.5,-0.2) rectangle (4.8,0.65);

        % Labels for phases
        \draw[decorate, decoration={brace, amplitude=5pt, raise=3pt}] (-5.5,2) -- (-2.5,2)
            node[midway, above=8pt, font=\scriptsize] {First Arm (tokens 1-9)};
        \draw[decorate, decoration={brace, amplitude=5pt, raise=3pt}] (-2,2) -- (1.5,2)
            node[midway, above=8pt, font=\scriptsize] {Second Arm (10-18)};
        \draw[decorate, decoration={brace, amplitude=5pt, raise=3pt}] (2,2) -- (5.5,2)
            node[midway, above=8pt, font=\scriptsize] {Third Arm (19-27)};

        % Legend
        \node[draw, fill=white, font=\scriptsize, align=left] at (0,-1.5) {
            \textbullet\ Each token: argmax($P(t_i | t_{<i}, \text{context})$)\\
            \textbullet\ 27 forward passes, KV cache reused\\
            \textbullet\ Output: 27 VQ tokens $\rightarrow$ 24$\times$20 actions
        };
    \end{tikzpicture}
    \end{center}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{Token to Action Conversion (1/2)}
    \textbf{Step 1: Extract Action Tokens}
    \begin{lstlisting}[language=Python]
# Find markers in generated sequence
quad_start = find_token(output, "<|quad_start|>")
quad_end = find_token(output, "<|quad_end|>")
action_tokens = output[quad_start+1 : quad_end]
    \end{lstlisting}

    \vspace{0.3cm}
    \textbf{Step 2: Convert VLA $\rightarrow$ VAE Token IDs}
    \begin{lstlisting}[language=Python]
vocab_size = 152064
vae_tokens = vocab_size - (action_tokens + 1)
vae_tokens = vae_tokens.clamp(0, 1023)
    \end{lstlisting}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{Token to Action Conversion (2/2)}
    \textbf{Step 3: Decode via VQVAE}
    \begin{lstlisting}[language=Python]
actions = vae.decode(vae_tokens)  # [B, 24, 20]
    \end{lstlisting}

    \vspace{0.3cm}
    \textbf{Step 4: Unnormalize}
    \begin{lstlisting}[language=Python]
actions = normalizer["action"].unnormalize(actions)
    \end{lstlisting}

    \vspace{0.5cm}
    \begin{block}{Output Specification}
        \begin{itemize}
            \item Shape: $[B, 24, 20]$ (batch, timesteps, action\_dim)
            \item Units: Physical coordinates (meters, radians, binary)
        \end{itemize}
    \end{block}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Valid Token Rate (1/2)}
    \begin{block}{Quality Metric}
        Percentage of generated tokens within valid codebook range
    \end{block}

    \vspace{0.3cm}
    \textbf{Definition:}
    \[
        \text{valid\_rate} = \frac{|\{t : 0 \leq t < 1024\}|}{27}
    \]

    \vspace{0.3cm}
    \textbf{Invalid Token Causes:}
    \begin{itemize}
        \item Model generates text tokens instead of action tokens
        \item Token ID outside codebook range after conversion
        \item Premature \code{<|quad\_end|>} generation
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Valid Token Rate (2/2)}
    \begin{alertblock}{Handling Invalid Tokens}
        \begin{itemize}
            \item Clamp to valid range: $[0, 1023]$
            \item Monitor valid\_rate during training/inference
            \item Low valid\_rate indicates training issues
        \end{itemize}
    \end{alertblock}

    \vspace{0.5cm}
    \begin{block}{Expected Values}
        \begin{itemize}
            \item Well-trained model: $> 99\%$ valid rate
            \item Early training: May have lower valid rate
            \item Zero-shot: Varies by embodiment similarity
        \end{itemize}
    \end{block}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Training: Loss Function}
    \textbf{Cross-Entropy Loss on Action Tokens:}

    \vspace{0.3cm}
    \[
        \mathcal{L}_{\text{CE}} = -\sum_{i=1}^{27} \log P(t_i | t_{<i}, \text{image}, \text{instruction})
    \]

    \vspace{0.5cm}
    \textbf{Label Preparation:}
    \begin{enumerate}
        \item Encode ground-truth actions: $\vect{a} \rightarrow \vect{t}_{\text{gt}}$ via VQVAE
        \item Convert to VLA token IDs: $t_{\text{vla}} = V - t_{\text{vae}} - 1$
        \item Mask non-action positions: $-100$ (ignored in loss)
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Training Configuration:}
    \begin{itemize}
        \item Only action token positions contribute to loss
        \item Image/text tokens are input context only
        \item Teacher forcing: ground-truth tokens as input
    \end{itemize}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{Training: LoRA Configuration (1/2)}
    \begin{block}{Parameter-Efficient Fine-Tuning}
        LoRA adapters on attention layers
    \end{block}

    \vspace{0.3cm}
    \textbf{Target Modules:}
    \begin{lstlisting}[language=Python]
LoraConfig(
    r=8,                    # LoRA rank
    lora_alpha=8,           # Scaling factor
    lora_dropout=0.1,       # Dropout rate
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    use_dora=False,
    init_lora_weights="gaussian"
)
    \end{lstlisting}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Training: LoRA Configuration (2/2)}
    \textbf{Trainable Parameters:}
    \begin{itemize}
        \item LoRA: $\sim$20M parameters (0.3\% of 7B)
        \item Full fine-tune: 7B parameters
    \end{itemize}

    \vspace{0.5cm}
    \begin{block}{LoRA Benefits}
        \begin{itemize}
            \item Reduces memory: 80GB $\rightarrow$ 32GB
            \item Faster training convergence
            \item Preserves base model capabilities
            \item Easy to merge or swap adapters
        \end{itemize}
    \end{block}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{Training: QLoRA Configuration}
    \begin{columns}[T]
        \begin{column}{0.55\textwidth}
            \begin{block}{Quantized LoRA}
                4-bit quantization + LoRA
            \end{block}

            \vspace{0.2cm}
            \textbf{BitsAndBytes Config:}
            \begin{lstlisting}[language=Python]
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=bfloat16
)
            \end{lstlisting}
        \end{column}
        \begin{column}{0.45\textwidth}
            \textbf{LoRA + DoRA:}
            \begin{lstlisting}[language=Python]
LoraConfig(
    use_dora=True
)
            \end{lstlisting}

            \vspace{0.2cm}
            \textbf{Memory:}
            \begin{itemize}
                \item Full: 80GB
                \item LoRA: 32GB
                \item QLoRA: 20GB
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Evaluation Metrics (1/2)}
    \textbf{Computed during training/validation:}

    \vspace{0.5cm}
    \begin{table}
        \centering
        \renewcommand{\arraystretch}{1.3}
        \begin{tabular}{ll}
            \toprule
            \textbf{Metric} & \textbf{Formula} \\
            \midrule
            Valid Rate & $\frac{\text{\# valid tokens}}{27}$ \\[0.3cm]
            MSE (Total) & $\|\hat{\vect{a}} - \vect{a}\|^2$ \\[0.3cm]
            MSE (Position) & $\|\hat{\vect{p}} - \vect{p}\|^2$ \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Evaluation Metrics (2/2)}
    \begin{table}
        \centering
        \renewcommand{\arraystretch}{1.3}
        \begin{tabular}{ll}
            \toprule
            \textbf{Metric} & \textbf{Formula} \\
            \midrule
            Geodesic (Rotation) & $\arccos\left(\frac{\text{tr}(\mat{R}^T \hat{\mat{R}}) - 1}{2}\right)$ \\[0.3cm]
            MSE (Gripper) & $\|\hat{g} - g\|^2$ \\
            \bottomrule
        \end{tabular}
    \end{table}

    \vspace{0.5cm}
    \begin{block}{Note}
        Metrics computed on unnormalized actions for interpretability
    \end{block}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{vLLM Acceleration (1/2)}
    \begin{block}{Purpose}
        Faster inference using optimized LLM serving
    \end{block}

    \vspace{0.3cm}
    \textbf{vLLM Configuration:}
    \begin{lstlisting}[language=Python]
from vllm import LLM, SamplingParams

llm = LLM(
    model=model_path,
    dtype=torch.bfloat16,
    tensor_parallel_size=1,
    enable_chunked_prefill=True,
    gpu_memory_utilization=0.90,
    max_model_len=2048,
    limit_mm_per_prompt={"image": 1}
)
    \end{lstlisting}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{vLLM Acceleration (2/2)}
    \textbf{Sampling Parameters:}
    \begin{lstlisting}[language=Python]
sampling_params = SamplingParams(
    max_tokens=29,
    temperature=0.0
)
    \end{lstlisting}

    \vspace{0.5cm}
    \begin{block}{Key vLLM Features}
        \begin{itemize}
            \item Continuous batching
            \item Paged attention (efficient KV cache)
            \item Chunked prefill for long prompts
            \item Multi-modal support
        \end{itemize}
    \end{block}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{vLLM Inference (1/2)}
    \textbf{Inference with vLLM:}

    \vspace{0.3cm}
    \begin{lstlisting}[language=Python]
# Prepare input
prompt = build_chat_template(image, instruction)

# Generate
outputs = llm.generate(
    prompts=[prompt],
    sampling_params=sampling_params,
    use_tqdm=False
)

# Extract tokens (no detokenization)
token_ids = outputs[0].outputs[0].token_ids
    \end{lstlisting}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{vLLM Inference (2/2)}
    \textbf{Performance Comparison:}
    \begin{itemize}
        \item Standard HuggingFace: $\sim$400ms per inference
        \item vLLM: $\sim$237ms per inference
        \item Speedup: $\sim$1.7$\times$
    \end{itemize}

    \vspace{0.5cm}
    \begin{alertblock}{Real-Time Feasibility}
        \begin{itemize}
            \item Action chunk: 0.8s (24 frames at 30Hz)
            \item vLLM inference: 237ms
            \item Ratio: 30\% of chunk duration
            \item \textbf{Real-time capable} with temporal action chunking
        \end{itemize}
    \end{alertblock}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{KV Cache Optimization}
    \begin{columns}[T]
        \begin{column}{0.45\textwidth}
            \begin{block}{Key Insight}
                Encoding is constant across tokens
            \end{block}

            \vspace{0.2cm}
            \textbf{Standard:}
            \begin{itemize}
                \item $27 \times$ full forward
            \end{itemize}

            \textbf{With KV Cache:}
            \begin{itemize}
                \item 1st: Full forward
                \item 2-27: Decode only
            \end{itemize}
        \end{column}
        \begin{column}{0.55\textwidth}
            \textbf{Implementation:}
            \begin{lstlisting}[language=Python]
# Initial encoding
out = model(**inputs, use_cache=True)
kv = out.past_key_values

# Subsequent generation
out = model(input_ids=new_token,
            past_key_values=kv)
            \end{lstlisting}
        \end{column}
    \end{columns}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{RDT2-VQ Strengths \& Weaknesses}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \textbf{Strengths:}
            \begin{itemize}
                \item Full VLM language understanding
                \item Superior instruction following
                \item Unified text-action interface
                \item Stable discrete training
                \item Leverages pretrained VLM
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Weaknesses:}
            \begin{itemize}
                \item 27 forward passes (slow)
                \item Discretization error
                \item High memory usage
                \item Sequential generation
                \item Cannot parallelize tokens
            \end{itemize}
        \end{column}
    \end{columns}

    \vspace{0.5cm}
    \begin{alertblock}{Latency Analysis}
        27 forward passes $\times$ $\sim$15ms = $\sim$400ms per action chunk \\
        Action chunk = 0.8s $\Rightarrow$ \textbf{50\% of real-time}
    \end{alertblock}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}[fragile]{HuggingFace Model}
    \textbf{Model:} \code{robotics-diffusion-transformer/RDT2-VQ}

    \vspace{0.3cm}
    \textbf{Loading:}
    \begin{lstlisting}[language=Python]
from transformers import (
    Qwen2_5_VLForConditionalGeneration, AutoProcessor
)

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "robotics-diffusion-transformer/RDT2-VQ",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2"
)

processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct"
)
    \end{lstlisting}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{Complete Inference Pipeline}
    \begin{center}
    \begin{tikzpicture}[
        box/.style={rectangle, draw, rounded corners, minimum width=1.6cm, minimum height=0.9cm, align=center, font=\tiny},
        arrow/.style={->, thick},
        scale=0.85, transform shape
    ]
        % Row 1: Input (Images and Instruction side by side)
        \node[box, fill=blue!20] (img) at (0,3.5) {Images\\$2 \times 384^2$};
        \node[box, fill=green!20] (inst) at (2.5,3.5) {Instruction};

        % Processor below inputs
        \node[box, fill=gray!30] (proc) at (1.25,2) {Processor};

        % Row 2: Model
        \node[box, fill=qwenblue!30, minimum width=4.5cm] (qwen) at (2.5,0.5) {Qwen2.5-VL-7B (27 forward passes)};

        % Row 3: Tokens
        \node[box, fill=vqpurple!30] (vla_tok) at (0,-1) {VLA Tokens\\{[}27{]}};
        \node[box, fill=gray!30] (convert) at (2.5,-1) {Convert\\$V - t - 1$};
        \node[box, fill=vqpurple!30] (vae_tok) at (5,-1) {VAE Tokens\\{[}27{]}};
        \node[box, fill=yellow!30] (vae) at (7.5,-1) {VQVAE\\Decode};
        \node[box, fill=actionorange!30] (norm_act) at (10,-1) {Normalized\\{[}24,20{]}};

        % Row 4: Output
        \node[box, fill=gray!30] (unnorm) at (10,-2.5) {Unnormalize};
        \node[box, fill=rdtgreen!30] (action) at (7.5,-2.5) {Action\\{[}24,20{]}};

        % Arrows - Images and Instruction both go to Processor
        \draw[arrow] (img) -- (proc);
        \draw[arrow] (inst) -- (proc);
        \draw[arrow] (proc) -- (qwen);
        \draw[arrow] (qwen) -- (vla_tok);
        \draw[arrow] (vla_tok) -- (convert);
        \draw[arrow] (convert) -- (vae_tok);
        \draw[arrow] (vae_tok) -- (vae);
        \draw[arrow] (vae) -- (norm_act);
        \draw[arrow] (norm_act) -- (unnorm);
        \draw[arrow] (unnorm) -- (action);
    \end{tikzpicture}
    \end{center}

    \vspace{0.3cm}
    \textbf{Total Latency:} $\sim$400ms (without vLLM) / $\sim$237ms (with vLLM)
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{RDT2-VQ Summary (1/2)}
    \begin{table}
        \centering
        \renewcommand{\arraystretch}{1.3}
        \begin{tabular}{ll}
            \toprule
            \textbf{Attribute} & \textbf{Value} \\
            \midrule
            Backbone & Qwen2.5-VL-7B-Instruct \\
            Output type & Discrete tokens \\
            Tokens per chunk & 27 \\
            Forward passes & 27 \\
            Action chunk & 24 frames (0.8s) \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

%------------------------------------------------------------------------------
\begin{frame}{RDT2-VQ Summary (2/2)}
    \textbf{Latency:} $\sim$400ms (standard) / $\sim$237ms (with vLLM)

    \vspace{0.5cm}
    \textbf{Training modes:} Full / LoRA / QLoRA

    \vspace{0.5cm}
    \begin{block}{Use Case}
        Best for tasks requiring strong language understanding and instruction following, where latency is acceptable
    \end{block}
\end{frame}

